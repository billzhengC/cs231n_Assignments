{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 29 days\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "  \"\"\"\n",
    "  Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "  it for the linear classifier. These are the same steps as we used for the\n",
    "  SVM, but condensed to a single function.  \n",
    "  \"\"\"\n",
    "  # Load the raw CIFAR-10 data\n",
    "  cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "  X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "  \n",
    "  # subsample the data\n",
    "  mask = range(num_training, num_training + num_validation)\n",
    "  X_val = X_train[mask]\n",
    "  y_val = y_train[mask]\n",
    "  mask = range(num_training)\n",
    "  X_train = X_train[mask]\n",
    "  y_train = y_train[mask]\n",
    "  mask = range(num_test)\n",
    "  X_test = X_test[mask]\n",
    "  y_test = y_test[mask]\n",
    "  mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "  X_dev = X_train[mask]\n",
    "  y_dev = y_train[mask]\n",
    "  \n",
    "  # Preprocessing: reshape the image data into rows\n",
    "  X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "  X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "  X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "  X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "  \n",
    "  # Normalize the data: subtract the mean image\n",
    "  mean_image = np.mean(X_train, axis = 0)\n",
    "  X_train -= mean_image\n",
    "  X_val -= mean_image\n",
    "  X_test -= mean_image\n",
    "  X_dev -= mean_image\n",
    "  \n",
    "  # add bias dimension and transform into columns\n",
    "  X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "  X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "  X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "  X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "  \n",
    "  return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print 'Train data shape: ', X_train.shape\n",
    "print 'Train labels shape: ', y_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Validation labels shape: ', y_val.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', y_test.shape\n",
    "print 'dev data shape: ', X_dev.shape\n",
    "print 'dev labels shape: ', y_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.372730\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print 'loss: %f' % loss\n",
    "print 'sanity check: %f' % (-np.log(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 1.002296 analytic: 1.002296, relative error: 2.157704e-08\n",
      "numerical: -0.575576 analytic: -0.575576, relative error: 2.287122e-08\n",
      "numerical: -1.862831 analytic: -1.862831, relative error: 6.685271e-09\n",
      "numerical: 1.362019 analytic: 1.362019, relative error: 1.145549e-08\n",
      "numerical: -2.044721 analytic: -2.044721, relative error: 6.531590e-09\n",
      "numerical: 1.170690 analytic: 1.170689, relative error: 2.471023e-08\n",
      "numerical: 1.479773 analytic: 1.479773, relative error: 7.435116e-09\n",
      "numerical: -2.530792 analytic: -2.530792, relative error: 1.636792e-08\n",
      "numerical: -0.932385 analytic: -0.932385, relative error: 2.072614e-08\n",
      "numerical: 1.108130 analytic: 1.108130, relative error: 5.115135e-08\n",
      "Turn on Regularization\n",
      "numerical: -1.620095 analytic: -1.620095, relative error: 3.215508e-09\n",
      "numerical: -0.369814 analytic: -0.369814, relative error: 1.282202e-07\n",
      "numerical: 1.732189 analytic: 1.732189, relative error: 4.762842e-08\n",
      "numerical: -1.015174 analytic: -1.015174, relative error: 2.897840e-09\n",
      "numerical: -0.665819 analytic: -0.665820, relative error: 4.660906e-08\n",
      "numerical: -2.493275 analytic: -2.493275, relative error: 2.091386e-08\n",
      "numerical: 1.803093 analytic: 1.803093, relative error: 3.070855e-09\n",
      "numerical: 1.244943 analytic: 1.244943, relative error: 7.999670e-09\n",
      "numerical: -1.385240 analytic: -1.385240, relative error: 1.262728e-08\n",
      "numerical: 1.182283 analytic: 1.182283, relative error: 5.735686e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "print(\"Turn on Regularization\")\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 1e2)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 1e2)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.372730e+00 computed in 0.116646s\n",
      "vectorized loss: 2.372730e+00 computed in 0.005384s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print 'naive loss: %e computed in %fs' % (loss_naive, toc - tic)\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print 'vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic)\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print 'Loss difference: %f' % np.abs(loss_naive - loss_vectorized)\n",
    "print 'Gradient difference: %f' % grad_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 158.062097\n",
      "iteration 100 / 1000: loss 22.201084\n",
      "iteration 200 / 1000: loss 4.649598\n",
      "iteration 300 / 1000: loss 2.384598\n",
      "iteration 400 / 1000: loss 2.055282\n",
      "iteration 500 / 1000: loss 2.017421\n",
      "iteration 600 / 1000: loss 1.893710\n",
      "iteration 700 / 1000: loss 1.909137\n",
      "iteration 800 / 1000: loss 1.947030\n",
      "iteration 900 / 1000: loss 1.960936\n",
      "lr 1.000000e-06 reg 1.000000e+04\n",
      "\n",
      "iteration 0 / 1000: loss 203.887605\n",
      "iteration 100 / 1000: loss 16.400683\n",
      "iteration 200 / 1000: loss 3.019287\n",
      "iteration 300 / 1000: loss 2.033222\n",
      "iteration 400 / 1000: loss 2.019216\n",
      "iteration 500 / 1000: loss 1.857852\n",
      "iteration 600 / 1000: loss 2.032185\n",
      "iteration 700 / 1000: loss 1.977184\n",
      "iteration 800 / 1000: loss 2.110942\n",
      "iteration 900 / 1000: loss 1.991499\n",
      "lr 1.000000e-06 reg 1.291550e+04\n",
      "\n",
      "iteration 0 / 1000: loss 261.760807\n",
      "iteration 100 / 1000: loss 10.739978\n",
      "iteration 200 / 1000: loss 2.296470\n",
      "iteration 300 / 1000: loss 2.016272\n",
      "iteration 400 / 1000: loss 1.949091\n",
      "iteration 500 / 1000: loss 1.904806\n",
      "iteration 600 / 1000: loss 1.938741\n",
      "iteration 700 / 1000: loss 1.926924\n",
      "iteration 800 / 1000: loss 2.016353\n",
      "iteration 900 / 1000: loss 1.947474\n",
      "lr 1.000000e-06 reg 1.668101e+04\n",
      "\n",
      "iteration 0 / 1000: loss 336.895486\n",
      "iteration 100 / 1000: loss 6.106222\n",
      "iteration 200 / 1000: loss 2.063193\n",
      "iteration 300 / 1000: loss 1.924193\n",
      "iteration 400 / 1000: loss 2.029692\n",
      "iteration 500 / 1000: loss 2.031603\n",
      "iteration 600 / 1000: loss 2.033835\n",
      "iteration 700 / 1000: loss 2.042545\n",
      "iteration 800 / 1000: loss 2.086574\n",
      "iteration 900 / 1000: loss 2.023849\n",
      "lr 1.000000e-06 reg 2.154435e+04\n",
      "\n",
      "iteration 0 / 1000: loss 435.766251\n",
      "iteration 100 / 1000: loss 3.484878\n",
      "iteration 200 / 1000: loss 2.031615\n",
      "iteration 300 / 1000: loss 2.036568\n",
      "iteration 400 / 1000: loss 2.104081\n",
      "iteration 500 / 1000: loss 1.983679\n",
      "iteration 600 / 1000: loss 2.090078\n",
      "iteration 700 / 1000: loss 2.043946\n",
      "iteration 800 / 1000: loss 2.011513\n",
      "iteration 900 / 1000: loss 2.013359\n",
      "lr 1.000000e-06 reg 2.782559e+04\n",
      "\n",
      "iteration 0 / 1000: loss 548.996876\n",
      "iteration 100 / 1000: loss 2.403996\n",
      "iteration 200 / 1000: loss 2.016456\n",
      "iteration 300 / 1000: loss 2.046952\n",
      "iteration 400 / 1000: loss 2.111603\n",
      "iteration 500 / 1000: loss 2.118551\n",
      "iteration 600 / 1000: loss 2.038621\n",
      "iteration 700 / 1000: loss 2.059933\n",
      "iteration 800 / 1000: loss 2.072888\n",
      "iteration 900 / 1000: loss 2.113794\n",
      "lr 1.000000e-06 reg 3.593814e+04\n",
      "\n",
      "iteration 0 / 1000: loss 710.547442\n",
      "iteration 100 / 1000: loss 2.158099\n",
      "iteration 200 / 1000: loss 2.083804\n",
      "iteration 300 / 1000: loss 2.109251\n",
      "iteration 400 / 1000: loss 2.084655\n",
      "iteration 500 / 1000: loss 2.089232\n",
      "iteration 600 / 1000: loss 2.104240\n",
      "iteration 700 / 1000: loss 2.095981\n",
      "iteration 800 / 1000: loss 1.999474\n",
      "iteration 900 / 1000: loss 2.113598\n",
      "lr 1.000000e-06 reg 4.641589e+04\n",
      "\n",
      "iteration 0 / 1000: loss 937.407123\n",
      "iteration 100 / 1000: loss 2.109992\n",
      "iteration 200 / 1000: loss 2.123806\n",
      "iteration 300 / 1000: loss 2.066644\n",
      "iteration 400 / 1000: loss 2.144350\n",
      "iteration 500 / 1000: loss 2.103816\n",
      "iteration 600 / 1000: loss 2.091623\n",
      "iteration 700 / 1000: loss 2.121518\n",
      "iteration 800 / 1000: loss 2.111979\n",
      "iteration 900 / 1000: loss 2.071799\n",
      "lr 1.000000e-06 reg 5.994843e+04\n",
      "\n",
      "iteration 0 / 1000: loss 1212.780241\n",
      "iteration 100 / 1000: loss 2.136623\n",
      "iteration 200 / 1000: loss 2.102701\n",
      "iteration 300 / 1000: loss 2.091739\n",
      "iteration 400 / 1000: loss 2.136004\n",
      "iteration 500 / 1000: loss 2.179104\n",
      "iteration 600 / 1000: loss 2.111807\n",
      "iteration 700 / 1000: loss 2.160226\n",
      "iteration 800 / 1000: loss 2.169852\n",
      "iteration 900 / 1000: loss 2.148776\n",
      "lr 1.000000e-06 reg 7.742637e+04\n",
      "\n",
      "iteration 0 / 1000: loss 1535.427082\n",
      "iteration 100 / 1000: loss 2.132675\n",
      "iteration 200 / 1000: loss 2.175889\n",
      "iteration 300 / 1000: loss 2.177906\n",
      "iteration 400 / 1000: loss 2.160395\n",
      "iteration 500 / 1000: loss 2.157965\n",
      "iteration 600 / 1000: loss 2.147009\n",
      "iteration 700 / 1000: loss 2.171931\n",
      "iteration 800 / 1000: loss 2.170978\n",
      "iteration 900 / 1000: loss 2.138798\n",
      "lr 1.000000e-06 reg 1.000000e+05\n",
      "\n",
      "iteration 0 / 1000: loss 159.344783\n",
      "iteration 100 / 1000: loss 13.208028\n",
      "iteration 200 / 1000: loss 2.799483\n",
      "iteration 300 / 1000: loss 2.023428\n",
      "iteration 400 / 1000: loss 1.995436\n",
      "iteration 500 / 1000: loss 1.948662\n",
      "iteration 600 / 1000: loss 2.032384\n",
      "iteration 700 / 1000: loss 1.906461\n",
      "iteration 800 / 1000: loss 1.917588\n",
      "iteration 900 / 1000: loss 2.018818\n",
      "lr 1.291550e-06 reg 1.000000e+04\n",
      "\n",
      "iteration 0 / 1000: loss 201.863721\n",
      "iteration 100 / 1000: loss 8.640169\n",
      "iteration 200 / 1000: loss 2.219335\n",
      "iteration 300 / 1000: loss 2.042323\n",
      "iteration 400 / 1000: loss 2.006585\n",
      "iteration 500 / 1000: loss 1.981684\n",
      "iteration 600 / 1000: loss 1.966812\n",
      "iteration 700 / 1000: loss 1.984112\n",
      "iteration 800 / 1000: loss 2.014673\n",
      "iteration 900 / 1000: loss 2.010180\n",
      "lr 1.291550e-06 reg 1.291550e+04\n",
      "\n",
      "iteration 0 / 1000: loss 264.460612\n",
      "iteration 100 / 1000: loss 5.267761\n",
      "iteration 200 / 1000: loss 2.000918\n",
      "iteration 300 / 1000: loss 2.027345\n",
      "iteration 400 / 1000: loss 1.955192\n",
      "iteration 500 / 1000: loss 1.993009\n",
      "iteration 600 / 1000: loss 2.003929\n",
      "iteration 700 / 1000: loss 2.016388\n",
      "iteration 800 / 1000: loss 2.012871\n",
      "iteration 900 / 1000: loss 1.989576\n",
      "lr 1.291550e-06 reg 1.668101e+04\n",
      "\n",
      "iteration 0 / 1000: loss 334.854745\n",
      "iteration 100 / 1000: loss 3.139890\n",
      "iteration 200 / 1000: loss 2.006105\n",
      "iteration 300 / 1000: loss 1.934084\n",
      "iteration 400 / 1000: loss 2.010245\n",
      "iteration 500 / 1000: loss 2.063035\n",
      "iteration 600 / 1000: loss 2.030599\n",
      "iteration 700 / 1000: loss 2.067170\n",
      "iteration 800 / 1000: loss 1.989692\n",
      "iteration 900 / 1000: loss 2.027041\n",
      "lr 1.291550e-06 reg 2.154435e+04\n",
      "\n",
      "iteration 0 / 1000: loss 435.272176\n",
      "iteration 100 / 1000: loss 2.321528\n",
      "iteration 200 / 1000: loss 2.118740\n",
      "iteration 300 / 1000: loss 2.142896\n",
      "iteration 400 / 1000: loss 2.051087\n",
      "iteration 500 / 1000: loss 1.957426\n",
      "iteration 600 / 1000: loss 2.047863\n",
      "iteration 700 / 1000: loss 2.076389\n",
      "iteration 800 / 1000: loss 2.034062\n",
      "iteration 900 / 1000: loss 2.033169\n",
      "lr 1.291550e-06 reg 2.782559e+04\n",
      "\n",
      "iteration 0 / 1000: loss 556.012597\n",
      "iteration 100 / 1000: loss 2.092046\n",
      "iteration 200 / 1000: loss 2.026529\n",
      "iteration 300 / 1000: loss 2.071373\n",
      "iteration 400 / 1000: loss 2.106671\n",
      "iteration 500 / 1000: loss 2.061148\n",
      "iteration 600 / 1000: loss 2.102292\n",
      "iteration 700 / 1000: loss 2.038995\n",
      "iteration 800 / 1000: loss 2.100714\n",
      "iteration 900 / 1000: loss 2.095180\n",
      "lr 1.291550e-06 reg 3.593814e+04\n",
      "\n",
      "iteration 0 / 1000: loss 715.758801\n",
      "iteration 100 / 1000: loss 2.086547\n",
      "iteration 200 / 1000: loss 2.088507\n",
      "iteration 300 / 1000: loss 2.024670\n",
      "iteration 400 / 1000: loss 2.106540\n",
      "iteration 500 / 1000: loss 2.089106\n",
      "iteration 600 / 1000: loss 2.116546\n",
      "iteration 700 / 1000: loss 2.106658\n",
      "iteration 800 / 1000: loss 2.084524\n",
      "iteration 900 / 1000: loss 2.049830\n",
      "lr 1.291550e-06 reg 4.641589e+04\n",
      "\n",
      "iteration 0 / 1000: loss 937.119430\n",
      "iteration 100 / 1000: loss 2.175566\n",
      "iteration 200 / 1000: loss 2.132472\n",
      "iteration 300 / 1000: loss 2.098781\n",
      "iteration 400 / 1000: loss 2.055044\n",
      "iteration 500 / 1000: loss 2.094626\n",
      "iteration 600 / 1000: loss 2.078979\n",
      "iteration 700 / 1000: loss 2.119754\n",
      "iteration 800 / 1000: loss 2.154420\n",
      "iteration 900 / 1000: loss 2.084984\n",
      "lr 1.291550e-06 reg 5.994843e+04\n",
      "\n",
      "iteration 0 / 1000: loss 1200.540453\n",
      "iteration 100 / 1000: loss 2.176002\n",
      "iteration 200 / 1000: loss 2.100188\n",
      "iteration 300 / 1000: loss 2.148523\n",
      "iteration 400 / 1000: loss 2.180388\n",
      "iteration 500 / 1000: loss 2.079922\n",
      "iteration 600 / 1000: loss 2.138179\n",
      "iteration 700 / 1000: loss 2.193284\n",
      "iteration 800 / 1000: loss 2.141999\n",
      "iteration 900 / 1000: loss 2.156192\n",
      "lr 1.291550e-06 reg 7.742637e+04\n",
      "\n",
      "iteration 0 / 1000: loss 1539.317079\n",
      "iteration 100 / 1000: loss 2.150332\n",
      "iteration 200 / 1000: loss 2.098326\n",
      "iteration 300 / 1000: loss 2.205538\n",
      "iteration 400 / 1000: loss 2.128725\n",
      "iteration 500 / 1000: loss 2.186182\n",
      "iteration 600 / 1000: loss 2.187209\n",
      "iteration 700 / 1000: loss 2.150938\n",
      "iteration 800 / 1000: loss 2.170509\n",
      "iteration 900 / 1000: loss 2.172768\n",
      "lr 1.291550e-06 reg 1.000000e+05\n",
      "\n",
      "iteration 0 / 1000: loss 161.629773\n",
      "iteration 100 / 1000: loss 7.273324\n",
      "iteration 200 / 1000: loss 2.146246\n",
      "iteration 300 / 1000: loss 2.054772\n",
      "iteration 400 / 1000: loss 1.959374\n",
      "iteration 500 / 1000: loss 2.009772\n",
      "iteration 600 / 1000: loss 1.985691\n",
      "iteration 700 / 1000: loss 1.992704\n",
      "iteration 800 / 1000: loss 1.889831\n",
      "iteration 900 / 1000: loss 1.973061\n",
      "lr 1.668101e-06 reg 1.000000e+04\n",
      "\n",
      "iteration 0 / 1000: loss 205.676158\n",
      "iteration 100 / 1000: loss 4.519107\n",
      "iteration 200 / 1000: loss 1.994593\n",
      "iteration 300 / 1000: loss 2.000491\n",
      "iteration 400 / 1000: loss 1.971183\n",
      "iteration 500 / 1000: loss 1.996186\n",
      "iteration 600 / 1000: loss 2.004547\n",
      "iteration 700 / 1000: loss 2.091528\n",
      "iteration 800 / 1000: loss 1.974827\n",
      "iteration 900 / 1000: loss 1.959089\n",
      "lr 1.668101e-06 reg 1.291550e+04\n",
      "\n",
      "iteration 0 / 1000: loss 263.247556\n",
      "iteration 100 / 1000: loss 2.877794\n",
      "iteration 200 / 1000: loss 2.007927\n",
      "iteration 300 / 1000: loss 2.001903\n",
      "iteration 400 / 1000: loss 2.042929\n",
      "iteration 500 / 1000: loss 2.001726\n",
      "iteration 600 / 1000: loss 1.900522\n",
      "iteration 700 / 1000: loss 1.968543\n",
      "iteration 800 / 1000: loss 2.037107\n",
      "iteration 900 / 1000: loss 1.985612\n",
      "lr 1.668101e-06 reg 1.668101e+04\n",
      "\n",
      "iteration 0 / 1000: loss 336.824377\n",
      "iteration 100 / 1000: loss 2.208804\n",
      "iteration 200 / 1000: loss 2.031186\n",
      "iteration 300 / 1000: loss 2.090431\n",
      "iteration 400 / 1000: loss 2.014884\n",
      "iteration 500 / 1000: loss 2.009801\n",
      "iteration 600 / 1000: loss 1.992986\n",
      "iteration 700 / 1000: loss 2.131272\n",
      "iteration 800 / 1000: loss 1.998475\n",
      "iteration 900 / 1000: loss 2.002926\n",
      "lr 1.668101e-06 reg 2.154435e+04\n",
      "\n",
      "iteration 0 / 1000: loss 432.296006\n",
      "iteration 100 / 1000: loss 2.143936\n",
      "iteration 200 / 1000: loss 2.074491\n",
      "iteration 300 / 1000: loss 2.065730\n",
      "iteration 400 / 1000: loss 2.041323\n",
      "iteration 500 / 1000: loss 2.092571\n",
      "iteration 600 / 1000: loss 2.052366\n",
      "iteration 700 / 1000: loss 2.012737\n",
      "iteration 800 / 1000: loss 2.000365\n",
      "iteration 900 / 1000: loss 2.020573\n",
      "lr 1.668101e-06 reg 2.782559e+04\n",
      "\n",
      "iteration 0 / 1000: loss 553.909097\n",
      "iteration 100 / 1000: loss 2.004813\n",
      "iteration 200 / 1000: loss 2.047034\n",
      "iteration 300 / 1000: loss 2.099450\n",
      "iteration 400 / 1000: loss 2.023045\n",
      "iteration 500 / 1000: loss 2.073321\n",
      "iteration 600 / 1000: loss 2.158537\n",
      "iteration 700 / 1000: loss 2.039270\n",
      "iteration 800 / 1000: loss 2.038925\n",
      "iteration 900 / 1000: loss 2.115100\n",
      "lr 1.668101e-06 reg 3.593814e+04\n",
      "\n",
      "iteration 0 / 1000: loss 721.337200\n",
      "iteration 100 / 1000: loss 2.073187\n",
      "iteration 200 / 1000: loss 2.123106\n",
      "iteration 300 / 1000: loss 2.084577\n",
      "iteration 400 / 1000: loss 2.082769\n",
      "iteration 500 / 1000: loss 2.117301\n",
      "iteration 600 / 1000: loss 2.065208\n",
      "iteration 700 / 1000: loss 2.084752\n",
      "iteration 800 / 1000: loss 2.086452\n",
      "iteration 900 / 1000: loss 2.120512\n",
      "lr 1.668101e-06 reg 4.641589e+04\n",
      "\n",
      "iteration 0 / 1000: loss 926.664352\n",
      "iteration 100 / 1000: loss 2.123073\n",
      "iteration 200 / 1000: loss 2.160647\n",
      "iteration 300 / 1000: loss 2.098278\n",
      "iteration 400 / 1000: loss 2.091482\n",
      "iteration 500 / 1000: loss 2.139583\n",
      "iteration 600 / 1000: loss 2.155845\n",
      "iteration 700 / 1000: loss 2.072363\n",
      "iteration 800 / 1000: loss 2.097046\n",
      "iteration 900 / 1000: loss 2.149404\n",
      "lr 1.668101e-06 reg 5.994843e+04\n",
      "\n",
      "iteration 0 / 1000: loss 1193.244305\n",
      "iteration 100 / 1000: loss 2.098112\n",
      "iteration 200 / 1000: loss 2.172111\n",
      "iteration 300 / 1000: loss 2.160185\n",
      "iteration 400 / 1000: loss 2.173325\n",
      "iteration 500 / 1000: loss 2.128451\n",
      "iteration 600 / 1000: loss 2.207135\n",
      "iteration 700 / 1000: loss 2.147666\n",
      "iteration 800 / 1000: loss 2.183725\n",
      "iteration 900 / 1000: loss 2.132367\n",
      "lr 1.668101e-06 reg 7.742637e+04\n",
      "\n",
      "iteration 0 / 1000: loss 1549.598715\n",
      "iteration 100 / 1000: loss 2.148829\n",
      "iteration 200 / 1000: loss 2.120370\n",
      "iteration 300 / 1000: loss 2.166085\n",
      "iteration 400 / 1000: loss 2.182790\n",
      "iteration 500 / 1000: loss 2.130583\n",
      "iteration 600 / 1000: loss 2.132508\n",
      "iteration 700 / 1000: loss 2.177871\n",
      "iteration 800 / 1000: loss 2.213916\n",
      "iteration 900 / 1000: loss 2.169355\n",
      "lr 1.668101e-06 reg 1.000000e+05\n",
      "\n",
      "iteration 0 / 1000: loss 159.704041\n",
      "iteration 100 / 1000: loss 3.938983\n",
      "iteration 200 / 1000: loss 1.970939\n",
      "iteration 300 / 1000: loss 2.026590\n",
      "iteration 400 / 1000: loss 2.015397\n",
      "iteration 500 / 1000: loss 2.010996\n",
      "iteration 600 / 1000: loss 1.998056\n",
      "iteration 700 / 1000: loss 1.996844\n",
      "iteration 800 / 1000: loss 1.967511\n",
      "iteration 900 / 1000: loss 1.970413\n",
      "lr 2.154435e-06 reg 1.000000e+04\n",
      "\n",
      "iteration 0 / 1000: loss 204.178419\n",
      "iteration 100 / 1000: loss 2.639517\n",
      "iteration 200 / 1000: loss 2.021768\n",
      "iteration 300 / 1000: loss 1.995234\n",
      "iteration 400 / 1000: loss 2.009197\n",
      "iteration 500 / 1000: loss 2.000506\n",
      "iteration 600 / 1000: loss 1.981454\n",
      "iteration 700 / 1000: loss 1.968802\n",
      "iteration 800 / 1000: loss 2.017140\n",
      "iteration 900 / 1000: loss 1.926859\n",
      "lr 2.154435e-06 reg 1.291550e+04\n",
      "\n",
      "iteration 0 / 1000: loss 261.727836\n",
      "iteration 100 / 1000: loss 2.243215\n",
      "iteration 200 / 1000: loss 1.974541\n",
      "iteration 300 / 1000: loss 1.975134\n",
      "iteration 400 / 1000: loss 2.141430\n",
      "iteration 500 / 1000: loss 1.971767\n",
      "iteration 600 / 1000: loss 1.970927\n",
      "iteration 700 / 1000: loss 2.053351\n",
      "iteration 800 / 1000: loss 2.019667\n",
      "iteration 900 / 1000: loss 1.919018\n",
      "lr 2.154435e-06 reg 1.668101e+04\n",
      "\n",
      "iteration 0 / 1000: loss 339.592845\n",
      "iteration 100 / 1000: loss 2.073070\n",
      "iteration 200 / 1000: loss 2.018309\n",
      "iteration 300 / 1000: loss 2.058362\n",
      "iteration 400 / 1000: loss 2.028009\n",
      "iteration 500 / 1000: loss 1.976053\n",
      "iteration 600 / 1000: loss 2.001437\n",
      "iteration 700 / 1000: loss 2.082081\n",
      "iteration 800 / 1000: loss 2.038647\n",
      "iteration 900 / 1000: loss 2.098212\n",
      "lr 2.154435e-06 reg 2.154435e+04\n",
      "\n",
      "iteration 0 / 1000: loss 438.004262\n",
      "iteration 100 / 1000: loss 2.050030\n",
      "iteration 200 / 1000: loss 2.016176\n",
      "iteration 300 / 1000: loss 2.117557\n",
      "iteration 400 / 1000: loss 2.074406\n",
      "iteration 500 / 1000: loss 2.074108\n",
      "iteration 600 / 1000: loss 2.012627\n",
      "iteration 700 / 1000: loss 2.052075\n",
      "iteration 800 / 1000: loss 2.083445\n",
      "iteration 900 / 1000: loss 2.098741\n",
      "lr 2.154435e-06 reg 2.782559e+04\n",
      "\n",
      "iteration 0 / 1000: loss 559.944450\n",
      "iteration 100 / 1000: loss 2.123222\n",
      "iteration 200 / 1000: loss 2.006466\n",
      "iteration 300 / 1000: loss 2.065339\n",
      "iteration 400 / 1000: loss 2.027404\n",
      "iteration 500 / 1000: loss 2.091046\n",
      "iteration 600 / 1000: loss 2.100941\n",
      "iteration 700 / 1000: loss 2.082971\n",
      "iteration 800 / 1000: loss 2.055115\n",
      "iteration 900 / 1000: loss 2.080017\n",
      "lr 2.154435e-06 reg 3.593814e+04\n",
      "\n",
      "iteration 0 / 1000: loss 717.811838\n",
      "iteration 100 / 1000: loss 2.123516\n",
      "iteration 200 / 1000: loss 2.107486\n",
      "iteration 300 / 1000: loss 2.138281\n",
      "iteration 400 / 1000: loss 2.131955\n",
      "iteration 500 / 1000: loss 2.103312\n",
      "iteration 600 / 1000: loss 2.055461\n",
      "iteration 700 / 1000: loss 2.088092\n",
      "iteration 800 / 1000: loss 2.144144\n",
      "iteration 900 / 1000: loss 2.059092\n",
      "lr 2.154435e-06 reg 4.641589e+04\n",
      "\n",
      "iteration 0 / 1000: loss 932.126121\n",
      "iteration 100 / 1000: loss 2.085218\n",
      "iteration 200 / 1000: loss 2.225625\n",
      "iteration 300 / 1000: loss 2.113028\n",
      "iteration 400 / 1000: loss 2.092190\n",
      "iteration 500 / 1000: loss 2.153638\n",
      "iteration 600 / 1000: loss 2.115227\n",
      "iteration 700 / 1000: loss 2.167016\n",
      "iteration 800 / 1000: loss 2.189687\n",
      "iteration 900 / 1000: loss 2.107437\n",
      "lr 2.154435e-06 reg 5.994843e+04\n",
      "\n",
      "iteration 0 / 1000: loss 1197.922636\n",
      "iteration 100 / 1000: loss 2.134195\n",
      "iteration 200 / 1000: loss 2.137069\n",
      "iteration 300 / 1000: loss 2.165773\n",
      "iteration 400 / 1000: loss 2.163680\n",
      "iteration 500 / 1000: loss 2.110356\n",
      "iteration 600 / 1000: loss 2.122410\n",
      "iteration 700 / 1000: loss 2.149246\n",
      "iteration 800 / 1000: loss 2.160368\n",
      "iteration 900 / 1000: loss 2.209057\n",
      "lr 2.154435e-06 reg 7.742637e+04\n",
      "\n",
      "iteration 0 / 1000: loss 1528.226593\n",
      "iteration 100 / 1000: loss 2.214138\n",
      "iteration 200 / 1000: loss 2.184905\n",
      "iteration 300 / 1000: loss 2.175916\n",
      "iteration 400 / 1000: loss 2.165969\n",
      "iteration 500 / 1000: loss 2.152583\n",
      "iteration 600 / 1000: loss 2.163004\n",
      "iteration 700 / 1000: loss 2.152905\n",
      "iteration 800 / 1000: loss 2.146777\n",
      "iteration 900 / 1000: loss 2.195816\n",
      "lr 2.154435e-06 reg 1.000000e+05\n",
      "\n",
      "iteration 0 / 1000: loss 158.733196\n",
      "iteration 100 / 1000: loss 2.416258\n",
      "iteration 200 / 1000: loss 1.955758\n",
      "iteration 300 / 1000: loss 1.940191\n",
      "iteration 400 / 1000: loss 1.896113\n",
      "iteration 500 / 1000: loss 1.942346\n",
      "iteration 600 / 1000: loss 2.036166\n",
      "iteration 700 / 1000: loss 1.963305\n",
      "iteration 800 / 1000: loss 1.951165\n",
      "iteration 900 / 1000: loss 1.993328\n",
      "lr 2.782559e-06 reg 1.000000e+04\n",
      "\n",
      "iteration 0 / 1000: loss 201.573819\n",
      "iteration 100 / 1000: loss 2.054992\n",
      "iteration 200 / 1000: loss 2.034708\n",
      "iteration 300 / 1000: loss 1.989187\n",
      "iteration 400 / 1000: loss 2.021905\n",
      "iteration 500 / 1000: loss 2.029189\n",
      "iteration 600 / 1000: loss 2.109195\n",
      "iteration 700 / 1000: loss 1.948617\n",
      "iteration 800 / 1000: loss 1.953271\n",
      "iteration 900 / 1000: loss 1.949337\n",
      "lr 2.782559e-06 reg 1.291550e+04\n",
      "\n",
      "iteration 0 / 1000: loss 261.250622\n",
      "iteration 100 / 1000: loss 2.080526\n",
      "iteration 200 / 1000: loss 2.090991\n",
      "iteration 300 / 1000: loss 1.997103\n",
      "iteration 400 / 1000: loss 2.019663\n",
      "iteration 500 / 1000: loss 2.070260\n",
      "iteration 600 / 1000: loss 1.939267\n",
      "iteration 700 / 1000: loss 1.983836\n",
      "iteration 800 / 1000: loss 1.958256\n",
      "iteration 900 / 1000: loss 2.087313\n",
      "lr 2.782559e-06 reg 1.668101e+04\n",
      "\n",
      "iteration 0 / 1000: loss 336.707613\n",
      "iteration 100 / 1000: loss 2.089985\n",
      "iteration 200 / 1000: loss 2.138340\n",
      "iteration 300 / 1000: loss 2.049433\n",
      "iteration 400 / 1000: loss 2.026167\n",
      "iteration 500 / 1000: loss 2.114835\n",
      "iteration 600 / 1000: loss 2.085123\n",
      "iteration 700 / 1000: loss 2.067350\n",
      "iteration 800 / 1000: loss 2.142978\n",
      "iteration 900 / 1000: loss 2.033586\n",
      "lr 2.782559e-06 reg 2.154435e+04\n",
      "\n",
      "iteration 0 / 1000: loss 430.620870\n",
      "iteration 100 / 1000: loss 2.096589\n",
      "iteration 200 / 1000: loss 2.005197\n",
      "iteration 300 / 1000: loss 2.049099\n",
      "iteration 400 / 1000: loss 2.086554\n",
      "iteration 500 / 1000: loss 2.132651\n",
      "iteration 600 / 1000: loss 2.157141\n",
      "iteration 700 / 1000: loss 2.098796\n",
      "iteration 800 / 1000: loss 2.084413\n",
      "iteration 900 / 1000: loss 2.020901\n",
      "lr 2.782559e-06 reg 2.782559e+04\n",
      "\n",
      "iteration 0 / 1000: loss 563.916754\n",
      "iteration 100 / 1000: loss 2.094684\n",
      "iteration 200 / 1000: loss 2.119335\n",
      "iteration 300 / 1000: loss 2.058857\n",
      "iteration 400 / 1000: loss 2.065628\n",
      "iteration 500 / 1000: loss 2.028804\n",
      "iteration 600 / 1000: loss 2.095562\n",
      "iteration 700 / 1000: loss 2.078069\n",
      "iteration 800 / 1000: loss 2.176927\n",
      "iteration 900 / 1000: loss 2.074743\n",
      "lr 2.782559e-06 reg 3.593814e+04\n",
      "\n",
      "iteration 0 / 1000: loss 717.003531\n",
      "iteration 100 / 1000: loss 2.075663\n",
      "iteration 200 / 1000: loss 2.128071\n",
      "iteration 300 / 1000: loss 2.132959\n",
      "iteration 400 / 1000: loss 2.165206\n",
      "iteration 500 / 1000: loss 2.171797\n",
      "iteration 600 / 1000: loss 2.157095\n",
      "iteration 700 / 1000: loss 2.159222\n",
      "iteration 800 / 1000: loss 2.213442\n",
      "iteration 900 / 1000: loss 2.119771\n",
      "lr 2.782559e-06 reg 4.641589e+04\n",
      "\n",
      "iteration 0 / 1000: loss 925.033617\n",
      "iteration 100 / 1000: loss 2.103757\n",
      "iteration 200 / 1000: loss 2.180211\n",
      "iteration 300 / 1000: loss 2.196834\n",
      "iteration 400 / 1000: loss 2.151014\n",
      "iteration 500 / 1000: loss 2.154423\n",
      "iteration 600 / 1000: loss 2.161982\n",
      "iteration 700 / 1000: loss 2.132385\n",
      "iteration 800 / 1000: loss 2.106030\n",
      "iteration 900 / 1000: loss 2.176153\n",
      "lr 2.782559e-06 reg 5.994843e+04\n",
      "\n",
      "iteration 0 / 1000: loss 1191.883562\n",
      "iteration 100 / 1000: loss 2.158861\n",
      "iteration 200 / 1000: loss 2.224975\n",
      "iteration 300 / 1000: loss 2.217632\n",
      "iteration 400 / 1000: loss 2.174611\n",
      "iteration 500 / 1000: loss 2.205178\n",
      "iteration 600 / 1000: loss 2.174984\n",
      "iteration 700 / 1000: loss 2.190748\n",
      "iteration 800 / 1000: loss 2.168933\n",
      "iteration 900 / 1000: loss 2.160675\n",
      "lr 2.782559e-06 reg 7.742637e+04\n",
      "\n",
      "iteration 0 / 1000: loss 1549.189310\n",
      "iteration 100 / 1000: loss 2.237359\n",
      "iteration 200 / 1000: loss 2.170958\n",
      "iteration 300 / 1000: loss 2.224908\n",
      "iteration 400 / 1000: loss 2.193228\n",
      "iteration 500 / 1000: loss 2.215062\n",
      "iteration 600 / 1000: loss 2.243310\n",
      "iteration 700 / 1000: loss 2.161945\n",
      "iteration 800 / 1000: loss 2.134242\n",
      "iteration 900 / 1000: loss 2.224736\n",
      "lr 2.782559e-06 reg 1.000000e+05\n",
      "\n",
      "iteration 0 / 1000: loss 159.499747\n",
      "iteration 100 / 1000: loss 2.102537\n",
      "iteration 200 / 1000: loss 2.058518\n",
      "iteration 300 / 1000: loss 1.933153\n",
      "iteration 400 / 1000: loss 2.118206\n",
      "iteration 500 / 1000: loss 2.071853\n",
      "iteration 600 / 1000: loss 1.976685\n",
      "iteration 700 / 1000: loss 2.006277\n",
      "iteration 800 / 1000: loss 2.042427\n",
      "iteration 900 / 1000: loss 2.018687\n",
      "lr 3.593814e-06 reg 1.000000e+04\n",
      "\n",
      "iteration 0 / 1000: loss 206.200535\n",
      "iteration 100 / 1000: loss 1.980937\n",
      "iteration 200 / 1000: loss 2.086940\n",
      "iteration 300 / 1000: loss 2.017832\n",
      "iteration 400 / 1000: loss 2.074483\n",
      "iteration 500 / 1000: loss 2.057854\n",
      "iteration 600 / 1000: loss 2.094835\n",
      "iteration 700 / 1000: loss 2.061408\n",
      "iteration 800 / 1000: loss 2.096430\n",
      "iteration 900 / 1000: loss 2.005924\n",
      "lr 3.593814e-06 reg 1.291550e+04\n",
      "\n",
      "iteration 0 / 1000: loss 259.766913\n",
      "iteration 100 / 1000: loss 2.049505\n",
      "iteration 200 / 1000: loss 2.066523\n",
      "iteration 300 / 1000: loss 2.116491\n",
      "iteration 400 / 1000: loss 2.110181\n",
      "iteration 500 / 1000: loss 2.028902\n",
      "iteration 600 / 1000: loss 2.046820\n",
      "iteration 700 / 1000: loss 2.152605\n",
      "iteration 800 / 1000: loss 2.029735\n",
      "iteration 900 / 1000: loss 2.084083\n",
      "lr 3.593814e-06 reg 1.668101e+04\n",
      "\n",
      "iteration 0 / 1000: loss 336.199217\n",
      "iteration 100 / 1000: loss 2.058169\n",
      "iteration 200 / 1000: loss 2.112718\n",
      "iteration 300 / 1000: loss 2.108624\n",
      "iteration 400 / 1000: loss 1.992979\n",
      "iteration 500 / 1000: loss 2.048921\n",
      "iteration 600 / 1000: loss 2.013265\n",
      "iteration 700 / 1000: loss 2.035769\n",
      "iteration 800 / 1000: loss 2.042259\n",
      "iteration 900 / 1000: loss 2.114308\n",
      "lr 3.593814e-06 reg 2.154435e+04\n",
      "\n",
      "iteration 0 / 1000: loss 428.883101\n",
      "iteration 100 / 1000: loss 2.089709\n",
      "iteration 200 / 1000: loss 2.101776\n",
      "iteration 300 / 1000: loss 2.012601\n",
      "iteration 400 / 1000: loss 2.081471\n",
      "iteration 500 / 1000: loss 2.165471\n",
      "iteration 600 / 1000: loss 2.116795\n",
      "iteration 700 / 1000: loss 2.122989\n",
      "iteration 800 / 1000: loss 2.120250\n",
      "iteration 900 / 1000: loss 2.083510\n",
      "lr 3.593814e-06 reg 2.782559e+04\n",
      "\n",
      "iteration 0 / 1000: loss 554.714913\n",
      "iteration 100 / 1000: loss 2.091347\n",
      "iteration 200 / 1000: loss 2.094232\n",
      "iteration 300 / 1000: loss 2.085507\n",
      "iteration 400 / 1000: loss 2.134568\n",
      "iteration 500 / 1000: loss 2.112286\n",
      "iteration 600 / 1000: loss 2.194482\n",
      "iteration 700 / 1000: loss 2.051957\n",
      "iteration 800 / 1000: loss 2.130802\n",
      "iteration 900 / 1000: loss 2.112677\n",
      "lr 3.593814e-06 reg 3.593814e+04\n",
      "\n",
      "iteration 0 / 1000: loss 723.642711\n",
      "iteration 100 / 1000: loss 2.079009\n",
      "iteration 200 / 1000: loss 2.147261\n",
      "iteration 300 / 1000: loss 2.098415\n",
      "iteration 400 / 1000: loss 2.101157\n",
      "iteration 500 / 1000: loss 2.145292\n",
      "iteration 600 / 1000: loss 2.154167\n",
      "iteration 700 / 1000: loss 2.207866\n",
      "iteration 800 / 1000: loss 2.144264\n",
      "iteration 900 / 1000: loss 2.193384\n",
      "lr 3.593814e-06 reg 4.641589e+04\n",
      "\n",
      "iteration 0 / 1000: loss 939.987019\n",
      "iteration 100 / 1000: loss 2.152195\n",
      "iteration 200 / 1000: loss 2.197930\n",
      "iteration 300 / 1000: loss 2.228391\n",
      "iteration 400 / 1000: loss 2.196672\n",
      "iteration 500 / 1000: loss 2.128260\n",
      "iteration 600 / 1000: loss 2.264915\n",
      "iteration 700 / 1000: loss 2.224658\n",
      "iteration 800 / 1000: loss 2.296496\n",
      "iteration 900 / 1000: loss 2.203434\n",
      "lr 3.593814e-06 reg 5.994843e+04\n",
      "\n",
      "iteration 0 / 1000: loss 1204.553717\n",
      "iteration 100 / 1000: loss 2.262607\n",
      "iteration 200 / 1000: loss 2.238830\n",
      "iteration 300 / 1000: loss 2.258192\n",
      "iteration 400 / 1000: loss 2.275600\n",
      "iteration 500 / 1000: loss 2.224608\n",
      "iteration 600 / 1000: loss 2.273912\n",
      "iteration 700 / 1000: loss 2.183843\n",
      "iteration 800 / 1000: loss 2.365325\n",
      "iteration 900 / 1000: loss 2.175788\n",
      "lr 3.593814e-06 reg 7.742637e+04\n",
      "\n",
      "iteration 0 / 1000: loss 1543.460856\n",
      "iteration 100 / 1000: loss 2.302759\n",
      "iteration 200 / 1000: loss 2.288052\n",
      "iteration 300 / 1000: loss 2.257906\n",
      "iteration 400 / 1000: loss 2.245162\n",
      "iteration 500 / 1000: loss 2.293016\n",
      "iteration 600 / 1000: loss 2.467768\n",
      "iteration 700 / 1000: loss 2.242017\n",
      "iteration 800 / 1000: loss 2.255421\n",
      "iteration 900 / 1000: loss 2.363231\n",
      "lr 3.593814e-06 reg 1.000000e+05\n",
      "\n",
      "iteration 0 / 1000: loss 158.423687\n",
      "iteration 100 / 1000: loss 2.043007\n",
      "iteration 200 / 1000: loss 2.187331\n",
      "iteration 300 / 1000: loss 2.101309\n",
      "iteration 400 / 1000: loss 2.040823\n",
      "iteration 500 / 1000: loss 2.261243\n",
      "iteration 600 / 1000: loss 1.908113\n",
      "iteration 700 / 1000: loss 1.986420\n",
      "iteration 800 / 1000: loss 2.107501\n",
      "iteration 900 / 1000: loss 2.105340\n",
      "lr 4.641589e-06 reg 1.000000e+04\n",
      "\n",
      "iteration 0 / 1000: loss 202.956551\n",
      "iteration 100 / 1000: loss 2.059279\n",
      "iteration 200 / 1000: loss 2.066218\n",
      "iteration 300 / 1000: loss 2.073984\n",
      "iteration 400 / 1000: loss 2.217603\n",
      "iteration 500 / 1000: loss 1.984726\n",
      "iteration 600 / 1000: loss 1.949997\n",
      "iteration 700 / 1000: loss 2.096256\n",
      "iteration 800 / 1000: loss 2.073502\n",
      "iteration 900 / 1000: loss 2.239673\n",
      "lr 4.641589e-06 reg 1.291550e+04\n",
      "\n",
      "iteration 0 / 1000: loss 263.124222\n",
      "iteration 100 / 1000: loss 2.202818\n",
      "iteration 200 / 1000: loss 2.125264\n",
      "iteration 300 / 1000: loss 2.228977\n",
      "iteration 400 / 1000: loss 2.128850\n",
      "iteration 500 / 1000: loss 2.209835\n",
      "iteration 600 / 1000: loss 2.154315\n",
      "iteration 700 / 1000: loss 2.079862\n",
      "iteration 800 / 1000: loss 2.120983\n",
      "iteration 900 / 1000: loss 2.125582\n",
      "lr 4.641589e-06 reg 1.668101e+04\n",
      "\n",
      "iteration 0 / 1000: loss 338.679159\n",
      "iteration 100 / 1000: loss 2.043085\n",
      "iteration 200 / 1000: loss 2.275999\n",
      "iteration 300 / 1000: loss 2.104971\n",
      "iteration 400 / 1000: loss 2.231452\n",
      "iteration 500 / 1000: loss 2.079168\n",
      "iteration 600 / 1000: loss 2.053190\n",
      "iteration 700 / 1000: loss 2.294422\n",
      "iteration 800 / 1000: loss 2.169374\n",
      "iteration 900 / 1000: loss 2.234933\n",
      "lr 4.641589e-06 reg 2.154435e+04\n",
      "\n",
      "iteration 0 / 1000: loss 427.000478\n",
      "iteration 100 / 1000: loss 2.169029\n",
      "iteration 200 / 1000: loss 2.244052\n",
      "iteration 300 / 1000: loss 2.129442\n",
      "iteration 400 / 1000: loss 2.198433\n",
      "iteration 500 / 1000: loss 2.070130\n",
      "iteration 600 / 1000: loss 2.422141\n",
      "iteration 700 / 1000: loss 2.190443\n",
      "iteration 800 / 1000: loss 2.201817\n",
      "iteration 900 / 1000: loss 2.355927\n",
      "lr 4.641589e-06 reg 2.782559e+04\n",
      "\n",
      "iteration 0 / 1000: loss 563.515497\n",
      "iteration 100 / 1000: loss 2.164502\n",
      "iteration 200 / 1000: loss 2.244593\n",
      "iteration 300 / 1000: loss 2.329896\n",
      "iteration 400 / 1000: loss 2.449647\n",
      "iteration 500 / 1000: loss 2.864018\n",
      "iteration 600 / 1000: loss 2.182316\n",
      "iteration 700 / 1000: loss 2.175610\n",
      "iteration 800 / 1000: loss 2.244106\n",
      "iteration 900 / 1000: loss 2.261991\n",
      "lr 4.641589e-06 reg 3.593814e+04\n",
      "\n",
      "iteration 0 / 1000: loss 715.563617\n",
      "iteration 100 / 1000: loss 2.135883\n",
      "iteration 200 / 1000: loss 2.358352\n",
      "iteration 300 / 1000: loss 2.299779\n",
      "iteration 400 / 1000: loss 2.289467\n",
      "iteration 500 / 1000: loss 2.344990\n",
      "iteration 600 / 1000: loss 2.240229\n",
      "iteration 700 / 1000: loss 2.589994\n",
      "iteration 800 / 1000: loss 2.144000\n",
      "iteration 900 / 1000: loss 2.180978\n",
      "lr 4.641589e-06 reg 4.641589e+04\n",
      "\n",
      "iteration 0 / 1000: loss 922.168195\n",
      "iteration 100 / 1000: loss 2.521390\n",
      "iteration 200 / 1000: loss 2.432544\n",
      "iteration 300 / 1000: loss 2.318812\n",
      "iteration 400 / 1000: loss 2.212843\n",
      "iteration 500 / 1000: loss 2.444934\n",
      "iteration 600 / 1000: loss 2.819715\n",
      "iteration 700 / 1000: loss 3.034775\n",
      "iteration 800 / 1000: loss 2.302596\n",
      "iteration 900 / 1000: loss 2.639975\n",
      "lr 4.641589e-06 reg 5.994843e+04\n",
      "\n",
      "iteration 0 / 1000: loss 1208.348454\n",
      "iteration 100 / 1000: loss 2.798548\n",
      "iteration 200 / 1000: loss 2.977255\n",
      "iteration 300 / 1000: loss 2.334179\n",
      "iteration 400 / 1000: loss 2.545201\n",
      "iteration 500 / 1000: loss 2.901675\n",
      "iteration 600 / 1000: loss 2.427111\n",
      "iteration 700 / 1000: loss 3.055413\n",
      "iteration 800 / 1000: loss 3.178751\n",
      "iteration 900 / 1000: loss 2.421684\n",
      "lr 4.641589e-06 reg 7.742637e+04\n",
      "\n",
      "iteration 0 / 1000: loss 1530.648727\n",
      "iteration 100 / 1000: loss 3.376681\n",
      "iteration 200 / 1000: loss 3.150242\n",
      "iteration 300 / 1000: loss 2.749363\n",
      "iteration 400 / 1000: loss 3.290153\n",
      "iteration 500 / 1000: loss 3.017010\n",
      "iteration 600 / 1000: loss 2.528879\n",
      "iteration 700 / 1000: loss 2.726564\n",
      "iteration 800 / 1000: loss 3.215929\n",
      "iteration 900 / 1000: loss 2.871136\n",
      "lr 4.641589e-06 reg 1.000000e+05\n",
      "\n",
      "iteration 0 / 1000: loss 160.621360\n",
      "iteration 100 / 1000: loss 2.580227\n",
      "iteration 200 / 1000: loss 2.507000\n",
      "iteration 300 / 1000: loss 2.180652\n",
      "iteration 400 / 1000: loss 2.351330\n",
      "iteration 500 / 1000: loss 2.515771\n",
      "iteration 600 / 1000: loss 2.430107\n",
      "iteration 700 / 1000: loss 2.814337\n",
      "iteration 800 / 1000: loss 2.056174\n",
      "iteration 900 / 1000: loss 2.306973\n",
      "lr 5.994843e-06 reg 1.000000e+04\n",
      "\n",
      "iteration 0 / 1000: loss 207.880732\n",
      "iteration 100 / 1000: loss 2.920068\n",
      "iteration 200 / 1000: loss 2.219350\n",
      "iteration 300 / 1000: loss 2.379084\n",
      "iteration 400 / 1000: loss 2.617529\n",
      "iteration 500 / 1000: loss 2.467380\n",
      "iteration 600 / 1000: loss 2.676984\n",
      "iteration 700 / 1000: loss 3.265697\n",
      "iteration 800 / 1000: loss 2.394025\n",
      "iteration 900 / 1000: loss 2.125824\n",
      "lr 5.994843e-06 reg 1.291550e+04\n",
      "\n",
      "iteration 0 / 1000: loss 261.237177\n",
      "iteration 100 / 1000: loss 2.557553\n",
      "iteration 200 / 1000: loss 2.263162\n",
      "iteration 300 / 1000: loss 2.645176\n",
      "iteration 400 / 1000: loss 2.515044\n",
      "iteration 500 / 1000: loss 2.394537\n",
      "iteration 600 / 1000: loss 2.662124\n",
      "iteration 700 / 1000: loss 2.388645\n",
      "iteration 800 / 1000: loss 2.795575\n",
      "iteration 900 / 1000: loss 2.677403\n",
      "lr 5.994843e-06 reg 1.668101e+04\n",
      "\n",
      "iteration 0 / 1000: loss 334.322727\n",
      "iteration 100 / 1000: loss 2.413380\n",
      "iteration 200 / 1000: loss 3.173855\n",
      "iteration 300 / 1000: loss 2.674169\n",
      "iteration 400 / 1000: loss 2.371614\n",
      "iteration 500 / 1000: loss 3.306537\n",
      "iteration 600 / 1000: loss 2.305258\n",
      "iteration 700 / 1000: loss 2.382053\n",
      "iteration 800 / 1000: loss 2.551297\n",
      "iteration 900 / 1000: loss 2.424957\n",
      "lr 5.994843e-06 reg 2.154435e+04\n",
      "\n",
      "iteration 0 / 1000: loss 436.855761\n",
      "iteration 100 / 1000: loss 2.908713\n",
      "iteration 200 / 1000: loss 3.012271\n",
      "iteration 300 / 1000: loss 3.152931\n",
      "iteration 400 / 1000: loss 2.959779\n",
      "iteration 500 / 1000: loss 3.541625\n",
      "iteration 600 / 1000: loss 3.095997\n",
      "iteration 700 / 1000: loss 3.255987\n",
      "iteration 800 / 1000: loss 2.942136\n",
      "iteration 900 / 1000: loss 3.222374\n",
      "lr 5.994843e-06 reg 2.782559e+04\n",
      "\n",
      "iteration 0 / 1000: loss 562.737734\n",
      "iteration 100 / 1000: loss 2.347665\n",
      "iteration 200 / 1000: loss 2.619018\n",
      "iteration 300 / 1000: loss 2.757040\n",
      "iteration 400 / 1000: loss 2.501530\n",
      "iteration 500 / 1000: loss 2.551542\n",
      "iteration 600 / 1000: loss 3.059742\n",
      "iteration 700 / 1000: loss 2.637089\n",
      "iteration 800 / 1000: loss 3.091155\n",
      "iteration 900 / 1000: loss 3.059524\n",
      "lr 5.994843e-06 reg 3.593814e+04\n",
      "\n",
      "iteration 0 / 1000: loss 723.501141\n",
      "iteration 100 / 1000: loss 2.990710\n",
      "iteration 200 / 1000: loss 3.173267\n",
      "iteration 300 / 1000: loss 4.503746\n",
      "iteration 400 / 1000: loss 3.730888\n",
      "iteration 500 / 1000: loss 3.903291\n",
      "iteration 600 / 1000: loss 3.294023\n",
      "iteration 700 / 1000: loss 3.389519\n",
      "iteration 800 / 1000: loss 3.401978\n",
      "iteration 900 / 1000: loss 4.396317\n",
      "lr 5.994843e-06 reg 4.641589e+04\n",
      "\n",
      "iteration 0 / 1000: loss 940.045564\n",
      "iteration 100 / 1000: loss 2.908748\n",
      "iteration 200 / 1000: loss 4.045200\n",
      "iteration 300 / 1000: loss 4.512450\n",
      "iteration 400 / 1000: loss 4.041740\n",
      "iteration 500 / 1000: loss 3.013629\n",
      "iteration 600 / 1000: loss 4.220299\n",
      "iteration 700 / 1000: loss 3.586034\n",
      "iteration 800 / 1000: loss 3.632570\n",
      "iteration 900 / 1000: loss 3.739567\n",
      "lr 5.994843e-06 reg 5.994843e+04\n",
      "\n",
      "iteration 0 / 1000: loss 1190.702215\n",
      "iteration 100 / 1000: loss 5.026887\n",
      "iteration 200 / 1000: loss 4.353837\n",
      "iteration 300 / 1000: loss 3.445998\n",
      "iteration 400 / 1000: loss 3.604810\n",
      "iteration 500 / 1000: loss 4.170570\n",
      "iteration 600 / 1000: loss 5.643094\n",
      "iteration 700 / 1000: loss 4.812534\n",
      "iteration 800 / 1000: loss 4.664555\n",
      "iteration 900 / 1000: loss 3.683670\n",
      "lr 5.994843e-06 reg 7.742637e+04\n",
      "\n",
      "iteration 0 / 1000: loss 1533.106828\n",
      "iteration 100 / 1000: loss 4.739158\n",
      "iteration 200 / 1000: loss 5.547380\n",
      "iteration 300 / 1000: loss 4.518250\n",
      "iteration 400 / 1000: loss 5.129328\n",
      "iteration 500 / 1000: loss 4.789129\n",
      "iteration 600 / 1000: loss 5.239823\n",
      "iteration 700 / 1000: loss 4.257395\n",
      "iteration 800 / 1000: loss 6.100126\n",
      "iteration 900 / 1000: loss 4.718213\n",
      "lr 5.994843e-06 reg 1.000000e+05\n",
      "\n",
      "iteration 0 / 1000: loss 160.612380\n",
      "iteration 100 / 1000: loss 3.960301\n",
      "iteration 200 / 1000: loss 3.438591\n",
      "iteration 300 / 1000: loss 2.700741\n",
      "iteration 400 / 1000: loss 3.191159\n",
      "iteration 500 / 1000: loss 3.381592\n",
      "iteration 600 / 1000: loss 2.757223\n",
      "iteration 700 / 1000: loss 2.760338\n",
      "iteration 800 / 1000: loss 3.678368\n",
      "iteration 900 / 1000: loss 2.468701\n",
      "lr 7.742637e-06 reg 1.000000e+04\n",
      "\n",
      "iteration 0 / 1000: loss 204.409327\n",
      "iteration 100 / 1000: loss 2.956416\n",
      "iteration 200 / 1000: loss 3.431280\n",
      "iteration 300 / 1000: loss 4.268981\n",
      "iteration 400 / 1000: loss 3.075510\n",
      "iteration 500 / 1000: loss 3.353975\n",
      "iteration 600 / 1000: loss 3.680230\n",
      "iteration 700 / 1000: loss 2.303411\n",
      "iteration 800 / 1000: loss 2.529192\n",
      "iteration 900 / 1000: loss 2.742746\n",
      "lr 7.742637e-06 reg 1.291550e+04\n",
      "\n",
      "iteration 0 / 1000: loss 257.951773\n",
      "iteration 100 / 1000: loss 2.535567\n",
      "iteration 200 / 1000: loss 3.690467\n",
      "iteration 300 / 1000: loss 4.146686\n",
      "iteration 400 / 1000: loss 3.152117\n",
      "iteration 500 / 1000: loss 3.737403\n",
      "iteration 600 / 1000: loss 4.783359\n",
      "iteration 700 / 1000: loss 3.584965\n",
      "iteration 800 / 1000: loss 4.246933\n",
      "iteration 900 / 1000: loss 3.766746\n",
      "lr 7.742637e-06 reg 1.668101e+04\n",
      "\n",
      "iteration 0 / 1000: loss 336.153515\n",
      "iteration 100 / 1000: loss 3.647694\n",
      "iteration 200 / 1000: loss 2.856861\n",
      "iteration 300 / 1000: loss 3.414728\n",
      "iteration 400 / 1000: loss 2.870543\n",
      "iteration 500 / 1000: loss 4.862488\n",
      "iteration 600 / 1000: loss 2.419372\n",
      "iteration 700 / 1000: loss 3.527635\n",
      "iteration 800 / 1000: loss 2.998233\n",
      "iteration 900 / 1000: loss 4.570549\n",
      "lr 7.742637e-06 reg 2.154435e+04\n",
      "\n",
      "iteration 0 / 1000: loss 431.295027\n",
      "iteration 100 / 1000: loss 4.886275\n",
      "iteration 200 / 1000: loss 4.902032\n",
      "iteration 300 / 1000: loss 3.481078\n",
      "iteration 400 / 1000: loss 5.280676\n",
      "iteration 500 / 1000: loss 2.921281\n",
      "iteration 600 / 1000: loss 2.997430\n",
      "iteration 700 / 1000: loss 3.406562\n",
      "iteration 800 / 1000: loss 4.874649\n",
      "iteration 900 / 1000: loss 4.411685\n",
      "lr 7.742637e-06 reg 2.782559e+04\n",
      "\n",
      "iteration 0 / 1000: loss 559.775961\n",
      "iteration 100 / 1000: loss 4.294958\n",
      "iteration 200 / 1000: loss 4.495445\n",
      "iteration 300 / 1000: loss 5.480432\n",
      "iteration 400 / 1000: loss 3.352991\n",
      "iteration 500 / 1000: loss 5.132669\n",
      "iteration 600 / 1000: loss 4.389742\n",
      "iteration 700 / 1000: loss 4.578789\n",
      "iteration 800 / 1000: loss 3.139849\n",
      "iteration 900 / 1000: loss 3.469835\n",
      "lr 7.742637e-06 reg 3.593814e+04\n",
      "\n",
      "iteration 0 / 1000: loss 725.236290\n",
      "iteration 100 / 1000: loss 4.534858\n",
      "iteration 200 / 1000: loss 5.522651\n",
      "iteration 300 / 1000: loss 5.206617\n",
      "iteration 400 / 1000: loss 4.464627\n",
      "iteration 500 / 1000: loss 4.761879\n",
      "iteration 600 / 1000: loss 5.421831\n",
      "iteration 700 / 1000: loss 4.361234\n",
      "iteration 800 / 1000: loss 4.062175\n",
      "iteration 900 / 1000: loss 4.507999\n",
      "lr 7.742637e-06 reg 4.641589e+04\n",
      "\n",
      "iteration 0 / 1000: loss 933.590002\n",
      "iteration 100 / 1000: loss 7.458142\n",
      "iteration 200 / 1000: loss 6.673824\n",
      "iteration 300 / 1000: loss 5.202119\n",
      "iteration 400 / 1000: loss 5.111316\n",
      "iteration 500 / 1000: loss 5.204830\n",
      "iteration 600 / 1000: loss 5.533308\n",
      "iteration 700 / 1000: loss 5.593904\n",
      "iteration 800 / 1000: loss 6.050916\n",
      "iteration 900 / 1000: loss 4.681421\n",
      "lr 7.742637e-06 reg 5.994843e+04\n",
      "\n",
      "iteration 0 / 1000: loss 1183.531913\n",
      "iteration 100 / 1000: loss 7.635032\n",
      "iteration 200 / 1000: loss 6.089726\n",
      "iteration 300 / 1000: loss 5.716754\n",
      "iteration 400 / 1000: loss 5.858612\n",
      "iteration 500 / 1000: loss 6.336233\n",
      "iteration 600 / 1000: loss 6.948509\n",
      "iteration 700 / 1000: loss 5.695171\n",
      "iteration 800 / 1000: loss 6.581855\n",
      "iteration 900 / 1000: loss 7.719300\n",
      "lr 7.742637e-06 reg 7.742637e+04\n",
      "\n",
      "iteration 0 / 1000: loss 1546.525411\n",
      "iteration 100 / 1000: loss 9.769482\n",
      "iteration 200 / 1000: loss 7.811692\n",
      "iteration 300 / 1000: loss 9.496404\n",
      "iteration 400 / 1000: loss 7.717038\n",
      "iteration 500 / 1000: loss 8.064015\n",
      "iteration 600 / 1000: loss 9.037568\n",
      "iteration 700 / 1000: loss 10.360325\n",
      "iteration 800 / 1000: loss 9.396593\n",
      "iteration 900 / 1000: loss 11.147413\n",
      "lr 7.742637e-06 reg 1.000000e+05\n",
      "\n",
      "iteration 0 / 1000: loss 160.753045\n",
      "iteration 100 / 1000: loss 5.190297\n",
      "iteration 200 / 1000: loss 3.809145\n",
      "iteration 300 / 1000: loss 4.460669\n",
      "iteration 400 / 1000: loss 6.030602\n",
      "iteration 500 / 1000: loss 4.414110\n",
      "iteration 600 / 1000: loss 3.127416\n",
      "iteration 700 / 1000: loss 4.619082\n",
      "iteration 800 / 1000: loss 3.140930\n",
      "iteration 900 / 1000: loss 3.924623\n",
      "lr 1.000000e-05 reg 1.000000e+04\n",
      "\n",
      "iteration 0 / 1000: loss 206.325325\n",
      "iteration 100 / 1000: loss 4.734483\n",
      "iteration 200 / 1000: loss 2.733503\n",
      "iteration 300 / 1000: loss 3.399367\n",
      "iteration 400 / 1000: loss 4.019083\n",
      "iteration 500 / 1000: loss 2.858877\n",
      "iteration 600 / 1000: loss 4.411207\n",
      "iteration 700 / 1000: loss 4.575156\n",
      "iteration 800 / 1000: loss 4.289819\n",
      "iteration 900 / 1000: loss 4.186911\n",
      "lr 1.000000e-05 reg 1.291550e+04\n",
      "\n",
      "iteration 0 / 1000: loss 259.467360\n",
      "iteration 100 / 1000: loss 4.611565\n",
      "iteration 200 / 1000: loss 5.373932\n",
      "iteration 300 / 1000: loss 5.947744\n",
      "iteration 400 / 1000: loss 5.171194\n",
      "iteration 500 / 1000: loss 7.090946\n",
      "iteration 600 / 1000: loss 4.907656\n",
      "iteration 700 / 1000: loss 4.792702\n",
      "iteration 800 / 1000: loss 3.866662\n",
      "iteration 900 / 1000: loss 4.306992\n",
      "lr 1.000000e-05 reg 1.668101e+04\n",
      "\n",
      "iteration 0 / 1000: loss 335.132144\n",
      "iteration 100 / 1000: loss 4.077706\n",
      "iteration 200 / 1000: loss 7.462181\n",
      "iteration 300 / 1000: loss 3.452539\n",
      "iteration 400 / 1000: loss 5.078377\n",
      "iteration 500 / 1000: loss 5.387940\n",
      "iteration 600 / 1000: loss 5.489249\n",
      "iteration 700 / 1000: loss 5.142235\n",
      "iteration 800 / 1000: loss 6.128019\n",
      "iteration 900 / 1000: loss 6.517478\n",
      "lr 1.000000e-05 reg 2.154435e+04\n",
      "\n",
      "iteration 0 / 1000: loss 436.965557\n",
      "iteration 100 / 1000: loss 9.285820\n",
      "iteration 200 / 1000: loss 3.791839\n",
      "iteration 300 / 1000: loss 4.092507\n",
      "iteration 400 / 1000: loss 4.482069\n",
      "iteration 500 / 1000: loss 6.382349\n",
      "iteration 600 / 1000: loss 5.745357\n",
      "iteration 700 / 1000: loss 5.373319\n",
      "iteration 800 / 1000: loss 5.252179\n",
      "iteration 900 / 1000: loss 4.406827\n",
      "lr 1.000000e-05 reg 2.782559e+04\n",
      "\n",
      "iteration 0 / 1000: loss 554.084498\n",
      "iteration 100 / 1000: loss 7.080108\n",
      "iteration 200 / 1000: loss 7.485853\n",
      "iteration 300 / 1000: loss 6.466754\n",
      "iteration 400 / 1000: loss 8.487461\n",
      "iteration 500 / 1000: loss 6.450142\n",
      "iteration 600 / 1000: loss 7.051779\n",
      "iteration 700 / 1000: loss 7.113189\n",
      "iteration 800 / 1000: loss 5.823154\n",
      "iteration 900 / 1000: loss 7.761637\n",
      "lr 1.000000e-05 reg 3.593814e+04\n",
      "\n",
      "iteration 0 / 1000: loss 718.696535\n",
      "iteration 100 / 1000: loss 8.181341\n",
      "iteration 200 / 1000: loss 9.324411\n",
      "iteration 300 / 1000: loss 6.702833\n",
      "iteration 400 / 1000: loss 7.194698\n",
      "iteration 500 / 1000: loss 9.448202\n",
      "iteration 600 / 1000: loss 5.765145\n",
      "iteration 700 / 1000: loss 8.204039\n",
      "iteration 800 / 1000: loss 6.202355\n",
      "iteration 900 / 1000: loss 10.121423\n",
      "lr 1.000000e-05 reg 4.641589e+04\n",
      "\n",
      "iteration 0 / 1000: loss 934.880791\n",
      "iteration 100 / 1000: loss 8.358434\n",
      "iteration 200 / 1000: loss 7.726598\n",
      "iteration 300 / 1000: loss 8.447949\n",
      "iteration 400 / 1000: loss 8.926456\n",
      "iteration 500 / 1000: loss 9.374097\n",
      "iteration 600 / 1000: loss 8.657417\n",
      "iteration 700 / 1000: loss 9.132745\n",
      "iteration 800 / 1000: loss 8.073182\n",
      "iteration 900 / 1000: loss 9.179263\n",
      "lr 1.000000e-05 reg 5.994843e+04\n",
      "\n",
      "iteration 0 / 1000: loss 1211.860756\n",
      "iteration 100 / 1000: loss 13.788057\n",
      "iteration 200 / 1000: loss 11.545428\n",
      "iteration 300 / 1000: loss 10.864153\n",
      "iteration 400 / 1000: loss 16.346322\n",
      "iteration 500 / 1000: loss 9.710005\n",
      "iteration 600 / 1000: loss 12.691712\n",
      "iteration 700 / 1000: loss 9.616045\n",
      "iteration 800 / 1000: loss 11.484006\n",
      "iteration 900 / 1000: loss 12.366729\n",
      "lr 1.000000e-05 reg 7.742637e+04\n",
      "\n",
      "iteration 0 / 1000: loss 1572.299584\n",
      "iteration 100 / 1000: loss 15.598169\n",
      "iteration 200 / 1000: loss 17.615243\n",
      "iteration 300 / 1000: loss 16.535283\n",
      "iteration 400 / 1000: loss 14.185282\n",
      "iteration 500 / 1000: loss 16.515117\n",
      "iteration 600 / 1000: loss 16.418895\n",
      "iteration 700 / 1000: loss 15.185200\n",
      "iteration 800 / 1000: loss 14.181988\n",
      "iteration 900 / 1000: loss 13.250788\n",
      "lr 1.000000e-05 reg 1.000000e+05\n",
      "\n",
      "lr 1.000000e-06 reg 1.000000e+04 train accuracy: 0.368571 val accuracy: 0.381000\n",
      "lr 1.000000e-06 reg 1.291550e+04 train accuracy: 0.358796 val accuracy: 0.380000\n",
      "lr 1.000000e-06 reg 1.668101e+04 train accuracy: 0.351469 val accuracy: 0.353000\n",
      "lr 1.000000e-06 reg 2.154435e+04 train accuracy: 0.346143 val accuracy: 0.358000\n",
      "lr 1.000000e-06 reg 2.782559e+04 train accuracy: 0.331367 val accuracy: 0.353000\n",
      "lr 1.000000e-06 reg 3.593814e+04 train accuracy: 0.332918 val accuracy: 0.352000\n",
      "lr 1.000000e-06 reg 4.641589e+04 train accuracy: 0.320102 val accuracy: 0.335000\n",
      "lr 1.000000e-06 reg 5.994843e+04 train accuracy: 0.321490 val accuracy: 0.331000\n",
      "lr 1.000000e-06 reg 7.742637e+04 train accuracy: 0.294796 val accuracy: 0.298000\n",
      "lr 1.000000e-06 reg 1.000000e+05 train accuracy: 0.295245 val accuracy: 0.317000\n",
      "lr 1.291550e-06 reg 1.000000e+04 train accuracy: 0.362041 val accuracy: 0.378000\n",
      "lr 1.291550e-06 reg 1.291550e+04 train accuracy: 0.355918 val accuracy: 0.362000\n",
      "lr 1.291550e-06 reg 1.668101e+04 train accuracy: 0.337449 val accuracy: 0.349000\n",
      "lr 1.291550e-06 reg 2.154435e+04 train accuracy: 0.335918 val accuracy: 0.350000\n",
      "lr 1.291550e-06 reg 2.782559e+04 train accuracy: 0.343306 val accuracy: 0.355000\n",
      "lr 1.291550e-06 reg 3.593814e+04 train accuracy: 0.328224 val accuracy: 0.332000\n",
      "lr 1.291550e-06 reg 4.641589e+04 train accuracy: 0.329490 val accuracy: 0.330000\n",
      "lr 1.291550e-06 reg 5.994843e+04 train accuracy: 0.296122 val accuracy: 0.294000\n",
      "lr 1.291550e-06 reg 7.742637e+04 train accuracy: 0.302224 val accuracy: 0.313000\n",
      "lr 1.291550e-06 reg 1.000000e+05 train accuracy: 0.282286 val accuracy: 0.306000\n",
      "lr 1.668101e-06 reg 1.000000e+04 train accuracy: 0.362592 val accuracy: 0.365000\n",
      "lr 1.668101e-06 reg 1.291550e+04 train accuracy: 0.359143 val accuracy: 0.374000\n",
      "lr 1.668101e-06 reg 1.668101e+04 train accuracy: 0.350347 val accuracy: 0.343000\n",
      "lr 1.668101e-06 reg 2.154435e+04 train accuracy: 0.347939 val accuracy: 0.357000\n",
      "lr 1.668101e-06 reg 2.782559e+04 train accuracy: 0.326898 val accuracy: 0.338000\n",
      "lr 1.668101e-06 reg 3.593814e+04 train accuracy: 0.326633 val accuracy: 0.334000\n",
      "lr 1.668101e-06 reg 4.641589e+04 train accuracy: 0.312755 val accuracy: 0.305000\n",
      "lr 1.668101e-06 reg 5.994843e+04 train accuracy: 0.320041 val accuracy: 0.319000\n",
      "lr 1.668101e-06 reg 7.742637e+04 train accuracy: 0.281224 val accuracy: 0.302000\n",
      "lr 1.668101e-06 reg 1.000000e+05 train accuracy: 0.270490 val accuracy: 0.287000\n",
      "lr 2.154435e-06 reg 1.000000e+04 train accuracy: 0.350776 val accuracy: 0.356000\n",
      "lr 2.154435e-06 reg 1.291550e+04 train accuracy: 0.334020 val accuracy: 0.356000\n",
      "lr 2.154435e-06 reg 1.668101e+04 train accuracy: 0.343122 val accuracy: 0.340000\n",
      "lr 2.154435e-06 reg 2.154435e+04 train accuracy: 0.319837 val accuracy: 0.337000\n",
      "lr 2.154435e-06 reg 2.782559e+04 train accuracy: 0.330653 val accuracy: 0.345000\n",
      "lr 2.154435e-06 reg 3.593814e+04 train accuracy: 0.294327 val accuracy: 0.310000\n",
      "lr 2.154435e-06 reg 4.641589e+04 train accuracy: 0.304388 val accuracy: 0.317000\n",
      "lr 2.154435e-06 reg 5.994843e+04 train accuracy: 0.308000 val accuracy: 0.307000\n",
      "lr 2.154435e-06 reg 7.742637e+04 train accuracy: 0.286388 val accuracy: 0.273000\n",
      "lr 2.154435e-06 reg 1.000000e+05 train accuracy: 0.264020 val accuracy: 0.270000\n",
      "lr 2.782559e-06 reg 1.000000e+04 train accuracy: 0.345204 val accuracy: 0.350000\n",
      "lr 2.782559e-06 reg 1.291550e+04 train accuracy: 0.348980 val accuracy: 0.343000\n",
      "lr 2.782559e-06 reg 1.668101e+04 train accuracy: 0.322327 val accuracy: 0.331000\n",
      "lr 2.782559e-06 reg 2.154435e+04 train accuracy: 0.327000 val accuracy: 0.332000\n",
      "lr 2.782559e-06 reg 2.782559e+04 train accuracy: 0.312796 val accuracy: 0.307000\n",
      "lr 2.782559e-06 reg 3.593814e+04 train accuracy: 0.321490 val accuracy: 0.327000\n",
      "lr 2.782559e-06 reg 4.641589e+04 train accuracy: 0.280796 val accuracy: 0.301000\n",
      "lr 2.782559e-06 reg 5.994843e+04 train accuracy: 0.270429 val accuracy: 0.277000\n",
      "lr 2.782559e-06 reg 7.742637e+04 train accuracy: 0.245204 val accuracy: 0.253000\n",
      "lr 2.782559e-06 reg 1.000000e+05 train accuracy: 0.247102 val accuracy: 0.261000\n",
      "lr 3.593814e-06 reg 1.000000e+04 train accuracy: 0.329796 val accuracy: 0.349000\n",
      "lr 3.593814e-06 reg 1.291550e+04 train accuracy: 0.337898 val accuracy: 0.348000\n",
      "lr 3.593814e-06 reg 1.668101e+04 train accuracy: 0.305714 val accuracy: 0.309000\n",
      "lr 3.593814e-06 reg 2.154435e+04 train accuracy: 0.338061 val accuracy: 0.348000\n",
      "lr 3.593814e-06 reg 2.782559e+04 train accuracy: 0.302327 val accuracy: 0.316000\n",
      "lr 3.593814e-06 reg 3.593814e+04 train accuracy: 0.289510 val accuracy: 0.297000\n",
      "lr 3.593814e-06 reg 4.641589e+04 train accuracy: 0.296429 val accuracy: 0.319000\n",
      "lr 3.593814e-06 reg 5.994843e+04 train accuracy: 0.248449 val accuracy: 0.273000\n",
      "lr 3.593814e-06 reg 7.742637e+04 train accuracy: 0.235918 val accuracy: 0.234000\n",
      "lr 3.593814e-06 reg 1.000000e+05 train accuracy: 0.247327 val accuracy: 0.249000\n",
      "lr 4.641589e-06 reg 1.000000e+04 train accuracy: 0.278939 val accuracy: 0.295000\n",
      "lr 4.641589e-06 reg 1.291550e+04 train accuracy: 0.336327 val accuracy: 0.336000\n",
      "lr 4.641589e-06 reg 1.668101e+04 train accuracy: 0.302735 val accuracy: 0.302000\n",
      "lr 4.641589e-06 reg 2.154435e+04 train accuracy: 0.275449 val accuracy: 0.288000\n",
      "lr 4.641589e-06 reg 2.782559e+04 train accuracy: 0.275531 val accuracy: 0.287000\n",
      "lr 4.641589e-06 reg 3.593814e+04 train accuracy: 0.271347 val accuracy: 0.291000\n",
      "lr 4.641589e-06 reg 4.641589e+04 train accuracy: 0.217531 val accuracy: 0.218000\n",
      "lr 4.641589e-06 reg 5.994843e+04 train accuracy: 0.219735 val accuracy: 0.214000\n",
      "lr 4.641589e-06 reg 7.742637e+04 train accuracy: 0.204939 val accuracy: 0.209000\n",
      "lr 4.641589e-06 reg 1.000000e+05 train accuracy: 0.183673 val accuracy: 0.202000\n",
      "lr 5.994843e-06 reg 1.000000e+04 train accuracy: 0.268041 val accuracy: 0.272000\n",
      "lr 5.994843e-06 reg 1.291550e+04 train accuracy: 0.268959 val accuracy: 0.284000\n",
      "lr 5.994843e-06 reg 1.668101e+04 train accuracy: 0.197776 val accuracy: 0.200000\n",
      "lr 5.994843e-06 reg 2.154435e+04 train accuracy: 0.204796 val accuracy: 0.201000\n",
      "lr 5.994843e-06 reg 2.782559e+04 train accuracy: 0.222061 val accuracy: 0.259000\n",
      "lr 5.994843e-06 reg 3.593814e+04 train accuracy: 0.209245 val accuracy: 0.209000\n",
      "lr 5.994843e-06 reg 4.641589e+04 train accuracy: 0.197735 val accuracy: 0.207000\n",
      "lr 5.994843e-06 reg 5.994843e+04 train accuracy: 0.136245 val accuracy: 0.146000\n",
      "lr 5.994843e-06 reg 7.742637e+04 train accuracy: 0.137347 val accuracy: 0.129000\n",
      "lr 5.994843e-06 reg 1.000000e+05 train accuracy: 0.115571 val accuracy: 0.130000\n",
      "lr 7.742637e-06 reg 1.000000e+04 train accuracy: 0.222082 val accuracy: 0.231000\n",
      "lr 7.742637e-06 reg 1.291550e+04 train accuracy: 0.202531 val accuracy: 0.198000\n",
      "lr 7.742637e-06 reg 1.668101e+04 train accuracy: 0.167551 val accuracy: 0.176000\n",
      "lr 7.742637e-06 reg 2.154435e+04 train accuracy: 0.181653 val accuracy: 0.187000\n",
      "lr 7.742637e-06 reg 2.782559e+04 train accuracy: 0.146531 val accuracy: 0.157000\n",
      "lr 7.742637e-06 reg 3.593814e+04 train accuracy: 0.149408 val accuracy: 0.165000\n",
      "lr 7.742637e-06 reg 4.641589e+04 train accuracy: 0.148939 val accuracy: 0.157000\n",
      "lr 7.742637e-06 reg 5.994843e+04 train accuracy: 0.144041 val accuracy: 0.146000\n",
      "lr 7.742637e-06 reg 7.742637e+04 train accuracy: 0.126980 val accuracy: 0.121000\n",
      "lr 7.742637e-06 reg 1.000000e+05 train accuracy: 0.125306 val accuracy: 0.122000\n",
      "lr 1.000000e-05 reg 1.000000e+04 train accuracy: 0.217408 val accuracy: 0.212000\n",
      "lr 1.000000e-05 reg 1.291550e+04 train accuracy: 0.166857 val accuracy: 0.175000\n",
      "lr 1.000000e-05 reg 1.668101e+04 train accuracy: 0.210816 val accuracy: 0.241000\n",
      "lr 1.000000e-05 reg 2.154435e+04 train accuracy: 0.201796 val accuracy: 0.195000\n",
      "lr 1.000000e-05 reg 2.782559e+04 train accuracy: 0.156653 val accuracy: 0.156000\n",
      "lr 1.000000e-05 reg 3.593814e+04 train accuracy: 0.154163 val accuracy: 0.148000\n",
      "lr 1.000000e-05 reg 4.641589e+04 train accuracy: 0.141633 val accuracy: 0.130000\n",
      "lr 1.000000e-05 reg 5.994843e+04 train accuracy: 0.152878 val accuracy: 0.164000\n",
      "lr 1.000000e-05 reg 7.742637e+04 train accuracy: 0.111490 val accuracy: 0.140000\n",
      "lr 1.000000e-05 reg 1.000000e+05 train accuracy: 0.095714 val accuracy: 0.095000\n",
      "best validation accuracy achieved during cross-validation: 0.381000. lr=1.000000e-05, reg=1.000000e+05\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 5e-7]\n",
    "regularization_strengths = [5e4, 1e8]\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "import copy, itertools\n",
    "# lr_list = np.linspace(-6,-5, num=10)\n",
    "# lr_list = np.power(10, lr_list)\n",
    "lr_list = np.logspace(-6,-5, num=10)\n",
    "# reg_list = np.linspace(4, 5, num=10)\n",
    "# reg_list = np.power(10, reg_list)\n",
    "reg_list = np.logspace(4,5, num=10)\n",
    "for lr, reg in itertools.product(lr_list, reg_list):\n",
    "    softmax = Softmax()\n",
    "    softmax.train(X_train, y_train, learning_rate=lr, reg=reg,\n",
    "                 num_iters=1000, verbose=True)\n",
    "    print'lr %e reg %e\\n' % (lr, reg)\n",
    "    train_acc = (softmax.predict(X_train)==y_train).mean()\n",
    "    val_acc = (softmax.predict(X_val)==y_val).mean()\n",
    "    results[(lr, reg)] = (train_acc, val_acc)   \n",
    "    \n",
    "    if val_acc>best_val:\n",
    "        best_val = val_acc\n",
    "        best_softmax = copy.deepcopy(softmax)\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print 'lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy)\n",
    "    \n",
    "print 'best validation accuracy achieved during cross-validation: %f. lr=%e, reg=%e' % (\n",
    "    best_val, lr, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.374000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print 'softmax on raw pixels final test set accuracy: %f' % (test_accuracy, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk8AAAF/CAYAAABQVS1eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3IH/C2fSP5iYqlfWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x104a13910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in xrange(10):\n",
    "  plt.subplot(2, 5, i + 1)\n",
    "  \n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "  plt.imshow(wimg.astype('uint8'))\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": false,
   "toc_threshold": 4,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
